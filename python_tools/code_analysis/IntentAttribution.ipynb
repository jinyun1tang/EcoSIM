{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb799a52-f6ec-476a-9195-7193acbdf6c0",
   "metadata": {},
   "source": [
    "# input/output annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564f9351-cbcf-4fbf-954e-217a3b4d4e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 639 data declarations:\n",
      "\n",
      "Found 28 data declarations:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#build the variable list and their comments\n",
    "import re\n",
    "\n",
    "def extract_plant_data_declarations(filename):\n",
    "    \"\"\"\n",
    "    Extract data declarations from Fortran type blocks in the plant API data file.\n",
    "    \n",
    "    Returns a list of dictionaries containing variable name and comment for each declaration.\n",
    "    \"\"\"\n",
    "    \n",
    "    declarations = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find all type blocks that start with 'type, public ::' and end with 'contains'\n",
    "    type_pattern = r'type,\\s*public\\s*::\\s*\\w+.*?contains'\n",
    "    type_blocks = re.findall(type_pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    for block in type_blocks:\n",
    "        # Extract individual declaration lines\n",
    "        lines = block.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines, type declaration, and contains\n",
    "            if not line or line.lower().startswith('type,') or line.lower().startswith('contains'):\n",
    "                continue\n",
    "            \n",
    "            # Look for variable declarations (real, integer, character, logical)\n",
    "            declaration_pattern = r'^\\s*(real|integer|character|logical)\\s*.*?::\\s*([^!\\s]+).*?(!.*)?$'\n",
    "            match = re.match(declaration_pattern, line, re.IGNORECASE)\n",
    "            \n",
    "            if match:\n",
    "                data_type = match.group(1)\n",
    "                variable_part = match.group(2).strip()\n",
    "                comment_part = match.group(3) if match.group(3) else \"\"\n",
    "                \n",
    "                # Extract variable name (before first parenthesis if present)\n",
    "                var_name_match = re.match(r'([^(\\s]+)', variable_part)\n",
    "                if var_name_match:\n",
    "                    variable_name = var_name_match.group(1)\n",
    "                    \n",
    "                    # Clean up comment (remove leading !)\n",
    "                    comment = comment_part.lstrip('!').strip() if comment_part else \"\"\n",
    "                    \n",
    "                    declarations.append({\n",
    "                        'variable_name': variable_name,\n",
    "                        'comment': comment,\n",
    "                        'full_declaration': line\n",
    "                    })\n",
    "    \n",
    "    return declarations\n",
    "\n",
    "def print_declarations(declarations):\n",
    "    \"\"\"Print the extracted declarations in a formatted way.\"\"\"\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, decl in enumerate(declarations, 1):\n",
    "        print(f\"{i:2d}. Variable: {decl['variable_name']} !{decl['comment']}\")\n",
    "\n",
    "# Main execution\n",
    "\n",
    "filename = '../../f90src/APIData/PlantAPIData.F90'\n",
    "\n",
    "try:\n",
    "    declarations = extract_plant_data_declarations(filename)\n",
    "    print(f\"Found {len(declarations)} data declarations:\\n\")    \n",
    "#    print_declarations(declarations)\n",
    "    \n",
    "    # Also return the list for further processing if needed\n",
    "#    print(\"List of extracted data:\")\n",
    "    decl_varname,decl_comment=[],[]\n",
    "    for decl in declarations:\n",
    "        decl_varname.append(decl['variable_name'])\n",
    "        decl_comment.append(decl['comment'])\n",
    "#        print(f\"  {decl['variable_name']} - {decl['comment']}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing file: {e}\")\n",
    "\n",
    "filename = '../../f90src/Modelpars/GrosubPars.F90'\n",
    "\n",
    "try:\n",
    "    declarations = extract_plant_data_declarations(filename)\n",
    "    print(f\"Found {len(declarations)} data declarations:\\n\")    \n",
    "#    print_declarations(declarations)\n",
    "    \n",
    "    # Also return the list for further processing if needed\n",
    "#    print(\"List of extracted data:\")\n",
    "    for decl in declarations:\n",
    "        decl_varname.append(decl['variable_name'])\n",
    "        decl_comment.append(decl['comment'])\n",
    "#        print(f\"  {decl['variable_name']} - {decl['comment']}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing file: {e}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a7a7dd3-78ee-4eee-a653-94c9b0dbe390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: ExtractsMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 22\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 5\n",
      "   1. SUBROUTINE: extracts (28 lines)\n",
      "   2. SUBROUTINE: TotalLitrFall (58 lines)\n",
      "   3. SUBROUTINE: CalcTotalLeafArea (27 lines)\n",
      "   4. SUBROUTINE: TotalGasandSoluteUptake (135 lines)\n",
      "   5. SUBROUTINE: ExtractCanopyFluxes (117 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: InitPlantMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 22\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 10\n",
      "   1. SUBROUTINE: StartPlants (83 lines)\n",
      "   2. SUBROUTINE: InitShootGrowth (50 lines)\n",
      "   3. SUBROUTINE: PlantLitterFraction (234 lines)\n",
      "   4. SUBROUTINE: PFTThermalAcclimation (45 lines)\n",
      "   5. SUBROUTINE: InitDimensionsandUptake (117 lines)\n",
      "   6. SUBROUTINE: InitPlantPhenoMorphoBio (207 lines)\n",
      "   7. SUBROUTINE: InitMassBalance (71 lines)\n",
      "   8. SUBROUTINE: InitPlantHeatWater (57 lines)\n",
      "   9. SUBROUTINE: InitRootMychorMorphoBio (121 lines)\n",
      "  10. SUBROUTINE: InitSeedMorphoBio (73 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: InitVegBGC.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 11\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 1\n",
      "   1. SUBROUTINE: InitIrradianceGeometry (60 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: NoduleBGCMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 16\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 2\n",
      "   1. SUBROUTINE: CanopyNoduleBiochemistry (385 lines)\n",
      "   2. SUBROUTINE: RootNodulBiochemistry (379 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: NutUptakeMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 24\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 13\n",
      "   1. SUBROUTINE: PlantNutientO2Uptake (36 lines)\n",
      "   2. SUBROUTINE: FoliarNutrientInterception (68 lines)\n",
      "   3. SUBROUTINE: RootMycoO2NutrientUptake (181 lines)\n",
      "   4. SUBROUTINE: ZeroNutrientUptake (52 lines)\n",
      "   5. SUBROUTINE: UptakeMineralPhosporhus (84 lines)\n",
      "   6. SUBROUTINE: UptakeNO3 (182 lines)\n",
      "   7. SUBROUTINE: UptakeNH4 (173 lines)\n",
      "   8. SUBROUTINE: UptakeHPO4 (160 lines)\n",
      "   9. SUBROUTINE: UptakeH2PO4 (157 lines)\n",
      "  10. SUBROUTINE: UptakeMineralNitrogen (74 lines)\n",
      "  11. SUBROUTINE: GetUptakeCapcity (98 lines)\n",
      "  12. SUBROUTINE: RootExudates (87 lines)\n",
      "  13. SUBROUTINE: SumNutrientUptake (83 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: PhotoSynsMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 15\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 3\n",
      "   1. SUBROUTINE: ComputeGPP_C3 (187 lines)\n",
      "   2. SUBROUTINE: ComputeGPP_C4 (213 lines)\n",
      "   3. SUBROUTINE: ComputeGPP (117 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: PlantBalMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 25\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 8\n",
      "   1. SUBROUTINE: SumPlantBiom (69 lines)\n",
      "   2. SUBROUTINE: SumPlantRootGas (49 lines)\n",
      "   3. SUBROUTINE: SumPlantBiomStates (102 lines)\n",
      "   4. SUBROUTINE: SumPlantBranchBiome (49 lines)\n",
      "   5. SUBROUTINE: ZeroGrosub (52 lines)\n",
      "   6. SUBROUTINE: SumRootBiome (60 lines)\n",
      "   7. SUBROUTINE: EnterPlantBalance (14 lines)\n",
      "   8. SUBROUTINE: ExitPlantBalance (49 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 2\n",
      "\n",
      "Processing file: PlantBranchMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 31\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 23\n",
      "   1. SUBROUTINE: GrowOneBranch (195 lines)\n",
      "   2. SUBROUTINE: WithdrawBranchLeaves (51 lines)\n",
      "   3. SUBROUTINE: BranchBiomAllocate (108 lines)\n",
      "   4. SUBROUTINE: UpdateBranchAllometry (107 lines)\n",
      "   5. SUBROUTINE: CalcPartitionCoeff (250 lines)\n",
      "   6. SUBROUTINE: UpdatePhotosynthates (81 lines)\n",
      "   7. SUBROUTINE: C4PhotoProductTransfer (93 lines)\n",
      "   8. SUBROUTINE: RemobizeLeafNodes (315 lines)\n",
      "   9. SUBROUTINE: RemobilizeLeafLayers (329 lines)\n",
      "  10. SUBROUTINE: AllocateLeaf2CanopyLayers (273 lines)\n",
      "  11. SUBROUTINE: LeafClassAllocation (63 lines)\n",
      "  12. SUBROUTINE: GrainFilling (249 lines)\n",
      "  13. SUBROUTINE: ResetNonAnnualBranch (209 lines)\n",
      "  14. SUBROUTINE: ResetBranchPhenology (188 lines)\n",
      "  15. SUBROUTINE: BranchElmntTransfer (162 lines)\n",
      "  16. SUBROUTINE: DevelopMainBranch (206 lines)\n",
      "  17. SUBROUTINE: ComputRAutoAfEmergence (174 lines)\n",
      "  18. SUBROUTINE: ComputRAutoB4Emergence (190 lines)\n",
      "  19. SUBROUTINE: GrowLeavesOnBranch (80 lines)\n",
      "  20. SUBROUTINE: GrowPetioleOnBranch (78 lines)\n",
      "  21. SUBROUTINE: GrowStalkOnBranch (83 lines)\n",
      "  22. SUBROUTINE: RemobilizeBranch (139 lines)\n",
      "  23. SUBROUTINE: SenescenceBranch (222 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: PlantDebugMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 14\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 1\n",
      "   1. SUBROUTINE: PrintRootTracer (17 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: PlantDisturbByFireMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 24\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 6\n",
      "   1. SUBROUTINE: InitPlantFireMod (4 lines)\n",
      "   2. SUBROUTINE: StageRootRemovalByFire (28 lines)\n",
      "   3. SUBROUTINE: RemoveRootByFire (34 lines)\n",
      "   4. SUBROUTINE: AbvGrndLiterFallByFire (116 lines)\n",
      "   5. SUBROUTINE: AbvgBiomRemovalByFire (27 lines)\n",
      "   6. SUBROUTINE: ApplyBiomRemovalByFire (67 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 2\n",
      "\n",
      "Processing file: PlantDisturbByGrazingMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 24\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 5\n",
      "   1. SUBROUTINE: AbvgBiomRemovalByGrazing (42 lines)\n",
      "   2. SUBROUTINE: RemoveStandDeadByGrazing (27 lines)\n",
      "   3. SUBROUTINE: ApplyBiomRemovalByGrazing (64 lines)\n",
      "   4. SUBROUTINE: GrazingPlant (222 lines)\n",
      "   5. SUBROUTINE: CutBranchNonstalByGrazing (61 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 2\n",
      "\n",
      "Processing file: PlantDisturbByTillageMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 15\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 3\n",
      "   1. SUBROUTINE: RemoveShootByTillage (242 lines)\n",
      "   2. SUBROUTINE: RemoveBiomByTillage (51 lines)\n",
      "   3. SUBROUTINE: RemoveRootsByTillage (199 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 2\n",
      "\n",
      "Processing file: PlantDisturbsMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 44\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 19\n",
      "   1. SUBROUTINE: InitPlantDisturbance (4 lines)\n",
      "   2. SUBROUTINE: RemoveBiomByMgmt (25 lines)\n",
      "   3. SUBROUTINE: RemoveStandingDead (51 lines)\n",
      "   4. SUBROUTINE: RemoveBiomassByDisturbance (40 lines)\n",
      "   5. SUBROUTINE: PlantDisturbance (32 lines)\n",
      "   6. SUBROUTINE: LiterfallByDisturbance (100 lines)\n",
      "   7. SUBROUTINE: AbvgBiomRemovalByDisturb (69 lines)\n",
      "   8. SUBROUTINE: ApplyDisturbanceBiomRemoval (106 lines)\n",
      "   9. SUBROUTINE: RemoveBiomByHarvest (213 lines)\n",
      "  10. SUBROUTINE: ResetCutBranch (76 lines)\n",
      "  11. SUBROUTINE: BranchCutPlantStalk (187 lines)\n",
      "  12. SUBROUTINE: BranchCutReprodOrgans (115 lines)\n",
      "  13. SUBROUTINE: CutBranchNonstructural (112 lines)\n",
      "  14. SUBROUTINE: CutBranchSheathPetole (130 lines)\n",
      "  15. SUBROUTINE: HarvestCanopy (143 lines)\n",
      "  16. SUBROUTINE: StageBranch4Cut (107 lines)\n",
      "  17. SUBROUTINE: CutPlant (117 lines)\n",
      "  18. SUBROUTINE: RootMaterialRemovalL (92 lines)\n",
      "  19. SUBROUTINE: HarvstUpdateRootStateL (103 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: PlantNonstElmDynMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 21\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 8\n",
      "   1. SUBROUTINE: WithinBranchElmTransfer (134 lines)\n",
      "   2. SUBROUTINE: RootMycoNonstTransfer (72 lines)\n",
      "   3. SUBROUTINE: SeasonStoreRootNonstTransfer (56 lines)\n",
      "   4. SUBROUTINE: ShootRootElmTransfer (212 lines)\n",
      "   5. SUBROUTINE: PlantNonstElmTransfer (43 lines)\n",
      "   6. SUBROUTINE: SeasonStoreShootTransfer (65 lines)\n",
      "   7. SUBROUTINE: StalkRsrvShootNonstTransfer (39 lines)\n",
      "   8. SUBROUTINE: StalkRsrvRootNonstTransfer (48 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 2\n",
      "\n",
      "Processing file: PlantPhenolMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 39\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 18\n",
      "   1. SUBROUTINE: hfuncs (68 lines)\n",
      "   2. SUBROUTINE: Emerged_plant_Phenology (54 lines)\n",
      "   3. SUBROUTINE: set_plant_flags (82 lines)\n",
      "   4. SUBROUTINE: root_shoot_branching (117 lines)\n",
      "   5. SUBROUTINE: FindMainBranchNumber (26 lines)\n",
      "   6. SUBROUTINE: StagePlantPhenology (99 lines)\n",
      "   7. SUBROUTINE: TestPlantEmergence (42 lines)\n",
      "   8. SUBROUTINE: branch_specific_phenology (119 lines)\n",
      "   9. SUBROUTINE: ColdDroughtDeciduPhenology (76 lines)\n",
      "  10. SUBROUTINE: CropEvergreenPhenology (50 lines)\n",
      "  11. SUBROUTINE: ColdDeciduousBranchPhenology (77 lines)\n",
      "  12. SUBROUTINE: live_branch_phenology (201 lines)\n",
      "  13. SUBROUTINE: InitBranchGrainFill (32 lines)\n",
      "  14. SUBROUTINE: BranchAnthesis (46 lines)\n",
      "  15. SUBROUTINE: BranchHeading (32 lines)\n",
      "  16. SUBROUTINE: BranchStemElongation (39 lines)\n",
      "  17. SUBROUTINE: BranchStemJointing (33 lines)\n",
      "  18. SUBROUTINE: InitiateBranchFlora (78 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: RootGasMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 18\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 1\n",
      "   1. SUBROUTINE: RootSoilGasExchange (598 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: RootMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 18\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 12\n",
      "   1. SUBROUTINE: RootBGCModel (95 lines)\n",
      "   2. SUBROUTINE: RootBiochemistry (241 lines)\n",
      "   3. SUBROUTINE: Grow2ndRootAxes (423 lines)\n",
      "   4. SUBROUTINE: GrowRootMycoAxes (128 lines)\n",
      "   5. SUBROUTINE: Grow1stRootAxes (369 lines)\n",
      "   6. SUBROUTINE: RootGroWaterDependence (25 lines)\n",
      "   7. SUBROUTINE: Withdraw2ndRoots (114 lines)\n",
      "   8. SUBROUTINE: RemobilizePrimeRoots (161 lines)\n",
      "   9. SUBROUTINE: ExtendPrimeRoots (159 lines)\n",
      "  10. SUBROUTINE: WithdrawPrimeRoots (150 lines)\n",
      "  11. SUBROUTINE: SummarizeRootSink (209 lines)\n",
      "  12. SUBROUTINE: RootCheck (35 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: StomatesMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 19\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 11\n",
      "   1. SUBROUTINE: StomatalDynamics (79 lines)\n",
      "   2. SUBROUTINE: C3FixCO2 (45 lines)\n",
      "   3. SUBROUTINE: C3PhotosynsCanopyLayerL (41 lines)\n",
      "   4. SUBROUTINE: C3Photosynthesis (87 lines)\n",
      "   5. SUBROUTINE: C4Photosynthesis (155 lines)\n",
      "   6. SUBROUTINE: C4PhotosynsCanopyLayerL (38 lines)\n",
      "   7. SUBROUTINE: C4FixCO2 (45 lines)\n",
      "   8. SUBROUTINE: LiveBranchPhotosynthesis (45 lines)\n",
      "   9. SUBROUTINE: PhenoActiveBranch (78 lines)\n",
      "  10. SUBROUTINE: PrepPhotosynthesis (72 lines)\n",
      "  11. SUBROUTINE: PhotoActivePFT (78 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n",
      "\n",
      "Processing file: SurfaceRadiationMod.F90\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "FILE SEPARATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "HEADER:\n",
      "  Lines: 26\n",
      "\n",
      "SUBROUTINES/FUNCTIONS: 6\n",
      "   1. SUBROUTINE: CanopyConditionModel (21 lines)\n",
      "   2. SUBROUTINE: CalcBoundaryLayerProperties (74 lines)\n",
      "   3. SUBROUTINE: DivideCanopyAreaByHeight (69 lines)\n",
      "   4. SUBROUTINE: SummaryCanopyAREA (78 lines)\n",
      "   5. SUBROUTINE: SurfaceRadiation (144 lines)\n",
      "   6. SUBROUTINE: MultiCanLayerRadiation (586 lines)\n",
      "\n",
      "TAIL:\n",
      "  Lines: 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "search_path = '../../f90src/Plant_bgc/'\n",
    "    \n",
    "files_to_process = ['ExtractsMod.F90','InitPlantMod.F90','InitVegBGC.F90','NoduleBGCMod.F90','NutUptakeMod.F90','PhotoSynsMod.F90','PlantBalMod.F90',\n",
    "    'PlantBranchMod.F90','PlantDebugMod.F90','PlantDisturbByFireMod.F90','PlantDisturbByGrazingMod.F90','PlantDisturbByTillageMod.F90',\n",
    "    'PlantDisturbsMod.F90','PlantNonstElmDynMod.F90','PlantPhenolMod.F90','RootGasMod.F90','RootMod.F90',\n",
    "    'StomatesMod.F90','SurfaceRadiationMod.F90']\n",
    "\n",
    "def separate_fortran_file(filename):\n",
    "    \"\"\"\n",
    "    Separate a Fortran file into header, subroutines/functions, and tail sections.\n",
    "    \n",
    "    Returns a dictionary containing:\n",
    "    - 'header': content from beginning to !header\n",
    "    - 'subroutines': list of subroutines/functions\n",
    "    - 'tail': content from !tail to end\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Initialize sections\n",
    "    header_lines = []\n",
    "    subroutines = []\n",
    "    tail_lines = []\n",
    "    \n",
    "    # State tracking\n",
    "    in_header = True\n",
    "    in_tail = False\n",
    "    current_subroutine = []\n",
    "    in_subroutine = False\n",
    "    subroutine_name = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Check for header end marker\n",
    "        if stripped_line.startswith('![header]') and in_header:\n",
    "            header_lines.append(line)\n",
    "            in_header = False\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for tail start marker\n",
    "        if stripped_line.startswith('![tail]'):\n",
    "            in_tail = True\n",
    "            tail_lines.append(line)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # If we're in header section\n",
    "        if in_header:\n",
    "            header_lines.append(line)\n",
    "        \n",
    "        # If we're in tail section\n",
    "        elif in_tail:\n",
    "            tail_lines.append(line)\n",
    "        \n",
    "        # Otherwise, we're in the main body - look for subroutines/functions\n",
    "        else:\n",
    "            # Check for subroutine start\n",
    "            subroutine_match = re.match(r'\\s*subroutine\\s+(\\w+)', stripped_line, re.IGNORECASE)\n",
    "            function_match = re.match(r'\\s*function\\s+(\\w+)', stripped_line, re.IGNORECASE)\n",
    "            \n",
    "            if subroutine_match or function_match:\n",
    "                # Save previous subroutine if exists\n",
    "                if current_subroutine:\n",
    "                    subroutines.append({\n",
    "                        'name': subroutine_name,\n",
    "                        'type': 'subroutine' if 'subroutine' in current_subroutine[0].lower() else 'function',\n",
    "                        'content': '\\n'.join(current_subroutine)\n",
    "                    })\n",
    "                \n",
    "                # Start new subroutine/function\n",
    "                current_subroutine = [line]\n",
    "                subroutine_name = subroutine_match.group(1) if subroutine_match else function_match.group(1)\n",
    "                in_subroutine = True\n",
    "            \n",
    "            elif in_subroutine:\n",
    "                current_subroutine.append(line)\n",
    "                \n",
    "                # Check for subroutine/function end\n",
    "                if (re.match(r'\\s*end\\s+subroutine', stripped_line, re.IGNORECASE) or \n",
    "                    re.match(r'\\s*end\\s+function', stripped_line, re.IGNORECASE)):\n",
    "                    \n",
    "                    # Save completed subroutine/function\n",
    "                    subroutines.append({\n",
    "                        'name': subroutine_name,\n",
    "                        'type': 'subroutine' if 'subroutine' in current_subroutine[0].lower() else 'function',\n",
    "                        'content': '\\n'.join(current_subroutine)\n",
    "                    })\n",
    "                    \n",
    "                    # Reset for next subroutine/function\n",
    "                    current_subroutine = []\n",
    "                    in_subroutine = False\n",
    "                    subroutine_name = \"\"\n",
    "            \n",
    "            else:\n",
    "                # Lines between subroutines (comments, blank lines, etc.)\n",
    "                # These could be added to a separate section if needed\n",
    "                pass\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Handle case where file ends while in a subroutine (shouldn't happen in well-formed code)\n",
    "    if current_subroutine and in_subroutine:\n",
    "        subroutines.append({\n",
    "            'name': subroutine_name,\n",
    "            'type': 'subroutine' if 'subroutine' in current_subroutine[0].lower() else 'function',\n",
    "            'content': '\\n'.join(current_subroutine)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'header': '\\n'.join(header_lines),\n",
    "        'subroutines': subroutines,\n",
    "        'tail': '\\n'.join(tail_lines)\n",
    "    }\n",
    "\n",
    "def write_separated_files(filename, sections, output_dir=None):\n",
    "    \"\"\"\n",
    "    Write the separated sections to individual files.\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(filename) or '.'\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    \n",
    "    # Write header\n",
    "    header_file = os.path.join(output_dir, f\"{base_name}_header.f90\")\n",
    "    with open(header_file, 'w') as f:\n",
    "        f.write(sections['header'])\n",
    "    print(f\"Header written to: {header_file}\")\n",
    "    \n",
    "    # Write each subroutine/function\n",
    "    for i, sub in enumerate(sections['subroutines']):\n",
    "        sub_file = os.path.join(output_dir, f\"{base_name}_{sub['type']}_{sub['name']}.f90\")\n",
    "        print('sub_file=%s'%sub_file)\n",
    "        with open(sub_file, 'w') as f:\n",
    "            f.write(sub['content'])\n",
    "        print(f\"{sub['type'].capitalize()} '{sub['name']}' written to: {sub_file}\")\n",
    "    \n",
    "    # Write tail\n",
    "    if sections['tail'].strip():  # Only write if tail is not empty\n",
    "        tail_file = os.path.join(output_dir, f\"{base_name}_tail.f90\")\n",
    "        with open(tail_file, 'w') as f:\n",
    "            f.write(sections['tail'])\n",
    "        print(f\"Tail written to: {tail_file}\")\n",
    "\n",
    "def print_summary(sections):\n",
    "    \"\"\"\n",
    "    Print a summary of the separated sections.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FILE SEPARATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nHEADER:\")\n",
    "    print(f\"  Lines: {len(sections['header'].split(chr(10)))}\")\n",
    "    \n",
    "    print(f\"\\nSUBROUTINES/FUNCTIONS: {len(sections['subroutines'])}\")\n",
    "    for i, sub in enumerate(sections['subroutines'], 1):\n",
    "        lines = len(sub['content'].split('\\n'))\n",
    "        print(f\"  {i:2d}. {sub['type'].upper()}: {sub['name']} ({lines} lines)\")\n",
    "    \n",
    "    print(f\"\\nTAIL:\")\n",
    "    print(f\"  Lines: {len(sections['tail'].split(chr(10)))}\")\n",
    "\n",
    "\n",
    "def separate_subroutine_function(content):\n",
    "    \"\"\"\n",
    "    Separate a subroutine or function into three parts:\n",
    "    1. From 'subroutine'/'function' to 'associate('\n",
    "    2. From 'associate(' to line with only ')'\n",
    "    3. Remaining content\n",
    "    \n",
    "    Returns a dictionary with the three parts or None if no associate block found.\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Find the associate statement\n",
    "    associate_start = -1\n",
    "    associate_end = -1\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Look for associate statement (case insensitive)\n",
    "        if re.match(r'\\s*associate\\s*\\(', line, re.IGNORECASE):\n",
    "            associate_start = i\n",
    "            continue\n",
    "        \n",
    "        # Look for the closing parenthesis (only non-space character on the line)\n",
    "        if associate_start != -1 and stripped == ')':\n",
    "            associate_end = i\n",
    "            break\n",
    "    \n",
    "    # If no associate block found, return the entire content as part 1\n",
    "    if associate_start == -1:\n",
    "        return {\n",
    "            'has_associate': False,\n",
    "            'part1': content,\n",
    "            'part2': '',\n",
    "            'part3': '',\n",
    "            'associate_start': -1,\n",
    "            'associate_end': -1\n",
    "        }\n",
    "    \n",
    "    # Split into three parts\n",
    "    part1_lines = lines[:associate_start]\n",
    "    part2_lines = lines[associate_start:associate_end + 1]\n",
    "    part3_lines = lines[associate_end + 1:]\n",
    "    \n",
    "    return {\n",
    "        'has_associate': True,\n",
    "        'part1': '\\n'.join(part1_lines),\n",
    "        'part2': '\\n'.join(part2_lines),\n",
    "        'part3': '\\n'.join(part3_lines),\n",
    "        'associate_start': associate_start,\n",
    "        'associate_end': associate_end\n",
    "    }\n",
    "\n",
    "def extract_and_separate_subroutines_functions(content):\n",
    "    \"\"\"\n",
    "    Extract all subroutines and functions from content and separate each into three parts.\n",
    "    \"\"\"\n",
    "        \n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    subroutines_functions = []\n",
    "    current_routine = []\n",
    "    in_routine = False\n",
    "    routine_name = \"\"\n",
    "    routine_type = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Check for subroutine start\n",
    "        subroutine_match = re.match(r'\\s*subroutine\\s+(\\w+)', stripped_line, re.IGNORECASE)\n",
    "        function_match = re.match(r'\\s*function\\s+(\\w+)', stripped_line, re.IGNORECASE)\n",
    "        \n",
    "        if subroutine_match or function_match:\n",
    "            # Save previous routine if exists\n",
    "            if current_routine:\n",
    "                routine_content = '\\n'.join(current_routine)\n",
    "                separated = separate_subroutine_function(routine_content)\n",
    "                subroutines_functions.append({\n",
    "                    'name': routine_name,\n",
    "                    'type': routine_type,\n",
    "                    'full_content': routine_content,\n",
    "                    'separated': separated\n",
    "                })\n",
    "            \n",
    "            # Start new routine\n",
    "            current_routine = [line]\n",
    "            routine_name = subroutine_match.group(1) if subroutine_match else function_match.group(1)\n",
    "            routine_type = 'subroutine' if subroutine_match else 'function'\n",
    "            in_routine = True\n",
    "        \n",
    "        elif in_routine:\n",
    "            current_routine.append(line)\n",
    "            \n",
    "            # Check for routine end\n",
    "            if (re.match(r'\\s*end\\s+subroutine', stripped_line, re.IGNORECASE) or \n",
    "                re.match(r'\\s*end\\s+function', stripped_line, re.IGNORECASE)):\n",
    "                \n",
    "                # Save completed routine\n",
    "                routine_content = '\\n'.join(current_routine)\n",
    "                separated = separate_subroutine_function(routine_content)\n",
    "                subroutines_functions.append({\n",
    "                    'name': routine_name,\n",
    "                    'type': routine_type,\n",
    "                    'full_content': routine_content,\n",
    "                    'separated': separated\n",
    "                })\n",
    "                \n",
    "                # Reset for next routine\n",
    "                current_routine = []\n",
    "                in_routine = False\n",
    "                routine_name = \"\"\n",
    "                routine_type = \"\"\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Handle case where file ends while in a routine\n",
    "    if current_routine and in_routine:\n",
    "        routine_content = '\\n'.join(current_routine)\n",
    "        separated = separate_subroutine_function(routine_content)\n",
    "        subroutines_functions.append({\n",
    "            'name': routine_name,\n",
    "            'type': routine_type,\n",
    "            'full_content': routine_content,\n",
    "            'separated': separated\n",
    "        })\n",
    "    \n",
    "    return subroutines_functions\n",
    "\n",
    "def write_separated_parts(filename, routines, output_dir=None):\n",
    "    \"\"\"\n",
    "    Write the separated parts to individual files.\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(filename) or '.'\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    \n",
    "    for routine in routines:\n",
    "        name = routine['name']\n",
    "        routine_type = routine['type']\n",
    "        separated = routine['separated']\n",
    "        \n",
    "        # Create a subdirectory for each routine\n",
    "        routine_dir = os.path.join(output_dir, f\"{base_name}_{routine_type}_{name}\")\n",
    "        os.makedirs(routine_dir, exist_ok=True)\n",
    "        \n",
    "        # Write part 1 (declaration to associate)\n",
    "        part1_file = os.path.join(routine_dir, f\"{name}_part1_declaration.f90\")\n",
    "        with open(part1_file, 'w') as f:\n",
    "            f.write(separated['part1'])\n",
    "        \n",
    "        if separated['has_associate']:\n",
    "            # Write part 2 (associate block)\n",
    "            part2_file = os.path.join(routine_dir, f\"{name}_part2_associate.f90\")\n",
    "            with open(part2_file, 'w') as f:\n",
    "                f.write(separated['part2'])\n",
    "            \n",
    "            # Write part 3 (remaining code)\n",
    "            part3_file = os.path.join(routine_dir, f\"{name}_part3_body.f90\")\n",
    "            with open(part3_file, 'w') as f:\n",
    "                f.write(separated['part3'])\n",
    "            \n",
    "            print(f\"{routine_type.capitalize()} '{name}' separated into 3 parts in: {routine_dir}\")\n",
    "        else:\n",
    "            print(f\"{routine_type.capitalize()} '{name}' has no associate block - only part 1 written to: {routine_dir}\")\n",
    "\n",
    "def print_separation_summary(routines):\n",
    "    \"\"\"\n",
    "    Print a summary of the separation results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUBROUTINE/FUNCTION SEPARATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_routines = len(routines)\n",
    "    routines_with_associate = sum(1 for r in routines if r['separated']['has_associate'])\n",
    "    \n",
    "    print(f\"\\nTotal routines found: {total_routines}\")\n",
    "    print(f\"Routines with associate blocks: {routines_with_associate}\")\n",
    "    print(f\"Routines without associate blocks: {total_routines - routines_with_associate}\")\n",
    "    \n",
    "    print(f\"\\nDetailed breakdown:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, routine in enumerate(routines, 1):\n",
    "        name = routine['name']\n",
    "        routine_type = routine['type']\n",
    "        separated = routine['separated']\n",
    "        \n",
    "        total_lines = len(routine['full_content'].split('\\n'))\n",
    "        \n",
    "        if separated['has_associate']:\n",
    "            part1_lines = len(separated['part1'].split('\\n'))\n",
    "            part2_lines = len(separated['part2'].split('\\n'))\n",
    "            part3_lines = len(separated['part3'].split('\\n'))\n",
    "            \n",
    "            print(f\"{i:2d}. {routine_type.upper()}: {name} (Total: {total_lines} lines)\")\n",
    "            print(f\"    Part 1 (Declaration): {part1_lines} lines\")\n",
    "            print(f\"    Part 2 (Associate):   {part2_lines} lines\")\n",
    "            print(f\"    Part 3 (Body):        {part3_lines} lines\")\n",
    "            print(f\"    Associate block: lines {separated['associate_start']+1}-{separated['associate_end']+1}\")\n",
    "        else:\n",
    "            print(f\"{i:2d}. {routine_type.upper()}: {name} (Total: {total_lines} lines)\")\n",
    "            print(f\"    No associate block found - entire content in Part 1\")\n",
    "        print()\n",
    "\n",
    "def analyze_associate_blocks(routines):\n",
    "    \"\"\"\n",
    "    Analyze the associate blocks found in the routines.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ASSOCIATE BLOCK ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for routine in routines:\n",
    "        if routine['separated']['has_associate']:\n",
    "            name = routine['name']\n",
    "            routine_type = routine['type']\n",
    "            part2 = routine['separated']['part2']\n",
    "            \n",
    "            print(f\"\\n{routine_type.upper()}: {name}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"Associate block content:\")\n",
    "            print(part2)\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "def extract_aliases_from_associate(part2_content):\n",
    "    \"\"\"\n",
    "    Extract variable aliases from the associate block.\n",
    "    Returns a dictionary mapping alias_var -> original_structure\n",
    "    \"\"\"\n",
    "    aliases = {}\n",
    "    \n",
    "    if not part2_content.strip():\n",
    "        return aliases\n",
    "    \n",
    "    lines = part2_content.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip associate( and ) lines\n",
    "        stripped = line.strip()\n",
    "        if (stripped.lower().startswith('associate(') or \n",
    "            stripped == ')' or \n",
    "            stripped == '' or\n",
    "            stripped.startswith('!')):\n",
    "            continue\n",
    "        \n",
    "        cleaned_line = re.sub(r'[,&\\s]*$', '', stripped)\n",
    "        \n",
    "        # Split by comma to handle multiple aliases on one line\n",
    "        # But be careful with commas that might be inside comments\n",
    "        alias_parts = []\n",
    "        if '!' in cleaned_line:\n",
    "            # Split at comment first, then handle the code part\n",
    "            code_part = cleaned_line.split('!')[0].strip()\n",
    "        else:\n",
    "            code_part = cleaned_line\n",
    "        \n",
    "        # Remove trailing comma and ampersand from code part\n",
    "        code_part = re.sub(r'[,&\\s]*$', '', code_part)\n",
    "        \n",
    "        # Split by comma for multiple aliases\n",
    "        parts = [part.strip() for part in code_part.split(',') if part.strip()]\n",
    "        alias_parts.extend(parts)\n",
    "        \n",
    "        for alias_part in alias_parts:\n",
    "            # Match pattern: alias => original, handling trailing comma/ampersand\n",
    "            # Remove any trailing comma, ampersand, or whitespace\n",
    "            clean_alias_part = re.sub(r'[,&\\s]*$', '', alias_part.strip())\n",
    "            \n",
    "            alias_match = re.match(r'(\\w+)\\s*=>\\s*(.+)', clean_alias_part)\n",
    "            if alias_match:\n",
    "                alias_var = alias_match.group(1).strip()\n",
    "                original_structure = alias_match.group(2).strip()\n",
    "                # Remove any trailing comma or ampersand from original_structure\n",
    "                original_structure = re.sub(r'[,&\\s]*$', '', original_structure)\n",
    "                aliases[alias_var] = original_structure    \n",
    "    return aliases\n",
    "\n",
    "def analyze_variable_usage_in_part3(part3_content, aliases):\n",
    "    \"\"\"\n",
    "    Analyze how each alias variable is used in part3.\n",
    "    Returns a dictionary with usage classification for each alias.\n",
    "    \"\"\"\n",
    "    usage_analysis = {}\n",
    "    \n",
    "    if not part3_content.strip():\n",
    "        return usage_analysis\n",
    "    \n",
    "    # Initialize analysis for each alias\n",
    "    for alias_var in aliases:\n",
    "        usage_analysis[alias_var] = {\n",
    "            'classification': 'unknown',\n",
    "            'first_occurrence': None,\n",
    "            'left_side_assignments': [],\n",
    "            'right_side_usages': [],\n",
    "            'function_calls': [],\n",
    "            'other_usages': [],\n",
    "            'all_occurrences': [],  # Track all occurrences in order\n",
    "            'both_sides_same_line': [],  # Track lines where var appears on both sides\n",
    "            'used_before_first_assignment': False\n",
    "        }\n",
    "    \n",
    "    lines = part3_content.split('\\n')\n",
    "    \n",
    "    # First pass: collect all occurrences in order\n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        # Skip comments and empty lines\n",
    "        stripped = line.strip()\n",
    "        if not stripped or stripped.startswith('!'):\n",
    "            continue\n",
    "        \n",
    "        # Check each alias variable\n",
    "        for alias_var in aliases:\n",
    "            if alias_var in line:\n",
    "                # Check if used in function call\n",
    "                func_pattern = rf'\\w+_func\\s*\\([^)]*{re.escape(alias_var)}[^)]*\\)'\n",
    "                if re.search(func_pattern, line, re.IGNORECASE):\n",
    "                    occurrence = {\n",
    "                        'line_num': line_num,\n",
    "                        'line': line.strip(),\n",
    "                        'type': 'function_call'\n",
    "                    }\n",
    "                    usage_analysis[alias_var]['function_calls'].append(occurrence)\n",
    "                    usage_analysis[alias_var]['all_occurrences'].append(occurrence)\n",
    "                    # Continue to check for other usages in the same line\n",
    "                \n",
    "                # Check for assignment statements\n",
    "                # Look for patterns like: var = ... or var(...) = ...\n",
    "                left_assign_pattern = rf'^\\s*{re.escape(alias_var)}(\\([^)]*\\))?\\s*='\n",
    "                is_left_assignment = re.search(left_assign_pattern, stripped)\n",
    "                \n",
    "                # Check if variable appears on right side of assignment\n",
    "                is_right_usage = False\n",
    "                is_other_usage = False\n",
    "                \n",
    "                if '=' in line:\n",
    "                    parts = line.split('=', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        right_part = parts[1]\n",
    "                        # Use word boundaries to avoid partial matches\n",
    "                        if re.search(rf'\\b{re.escape(alias_var)}\\b', right_part):\n",
    "                            is_right_usage = True\n",
    "                \n",
    "                # Check for both sides usage in same line\n",
    "                if is_left_assignment and is_right_usage:\n",
    "                    usage_analysis[alias_var]['both_sides_same_line'].append(line_num)\n",
    "                \n",
    "                # If not in assignment, it's other usage\n",
    "                if not is_left_assignment and not is_right_usage and re.search(rf'\\b{re.escape(alias_var)}\\b', line):\n",
    "                    # But skip if this usage is only in function call\n",
    "                    if not re.search(func_pattern, line, re.IGNORECASE):\n",
    "                        is_other_usage = True\n",
    "                \n",
    "                # Record occurrences (excluding function calls for classification purposes)\n",
    "                if is_left_assignment:\n",
    "                    occurrence = {\n",
    "                        'line_num': line_num,\n",
    "                        'line': line.strip(),\n",
    "                        'type': 'left_assignment'\n",
    "                    }\n",
    "                    usage_analysis[alias_var]['left_side_assignments'].append(occurrence)\n",
    "                    usage_analysis[alias_var]['all_occurrences'].append(occurrence)\n",
    "                \n",
    "                if is_right_usage:\n",
    "                    occurrence = {\n",
    "                        'line_num': line_num,\n",
    "                        'line': line.strip(),\n",
    "                        'type': 'right_usage'\n",
    "                    }\n",
    "                    usage_analysis[alias_var]['right_side_usages'].append(occurrence)\n",
    "                    usage_analysis[alias_var]['all_occurrences'].append(occurrence)\n",
    "                \n",
    "                if is_other_usage:\n",
    "                    occurrence = {\n",
    "                        'line_num': line_num,\n",
    "                        'line': line.strip(),\n",
    "                        'type': 'other'\n",
    "                    }\n",
    "                    usage_analysis[alias_var]['other_usages'].append(occurrence)\n",
    "                    usage_analysis[alias_var]['all_occurrences'].append(occurrence)\n",
    "                \n",
    "                # Set first occurrence if not set (excluding function calls)\n",
    "                if (usage_analysis[alias_var]['first_occurrence'] is None and \n",
    "                    (is_left_assignment or is_right_usage or is_other_usage)):\n",
    "                    if is_left_assignment:\n",
    "                        usage_analysis[alias_var]['first_occurrence'] = {\n",
    "                            'line_num': line_num,\n",
    "                            'type': 'left_assignment'\n",
    "                        }\n",
    "                    elif is_right_usage:\n",
    "                        usage_analysis[alias_var]['first_occurrence'] = {\n",
    "                            'line_num': line_num,\n",
    "                            'type': 'right_usage'\n",
    "                        }\n",
    "                    elif is_other_usage:\n",
    "                        usage_analysis[alias_var]['first_occurrence'] = {\n",
    "                            'line_num': line_num,\n",
    "                            'type': 'other'\n",
    "                        }\n",
    "    \n",
    "    # Second pass: analyze usage patterns and classify variables\n",
    "    for alias_var in aliases:\n",
    "        analysis = usage_analysis[alias_var]\n",
    "        \n",
    "        # Check if variable is not found in part3 (excluding function calls)\n",
    "        total_usages = (len(analysis['left_side_assignments']) + \n",
    "                       len(analysis['right_side_usages']) + \n",
    "                       len(analysis['other_usages']))\n",
    "        \n",
    "        # If only function calls or no usage at all, classify as unknown\n",
    "        if total_usages == 0:\n",
    "            analysis['classification'] = 'unknown'\n",
    "            continue\n",
    "        \n",
    "        # Sort all occurrences by line number to get chronological order (excluding function calls)\n",
    "        all_occurrences = sorted([occ for occ in analysis['all_occurrences'] \n",
    "                                if occ['type'] != 'function_call'], key=lambda x: x['line_num'])\n",
    "        \n",
    "        # Check if variable has been used before its first left assignment\n",
    "        first_left_assignment_line = None\n",
    "        for occ in all_occurrences:\n",
    "            if occ['type'] == 'left_assignment':\n",
    "                first_left_assignment_line = occ['line_num']\n",
    "                break\n",
    "        \n",
    "        if first_left_assignment_line:\n",
    "            for occ in all_occurrences:\n",
    "                if (occ['line_num'] < first_left_assignment_line and \n",
    "                    occ['type'] in ['right_usage', 'other']):\n",
    "                    analysis['used_before_first_assignment'] = True\n",
    "                    break\n",
    "        \n",
    "        # Classification logic:\n",
    "        # INOPUT: Variable appears on both sides of = in same line OR \n",
    "        #         variable is found on left side of = but has been used before\n",
    "        # OUTPUT: Variable is first found on left of = and doesn't meet inoput criteria\n",
    "        # INPUT: Any other usage pattern\n",
    "        # UNKNOWN: Not found in part3 OR only appears in function calls\n",
    "        \n",
    "        # Check for inoput conditions\n",
    "        has_both_sides_same_line = len(analysis['both_sides_same_line']) > 0\n",
    "        has_left_but_used_before = (len(analysis['left_side_assignments']) > 0 and \n",
    "                                   analysis['used_before_first_assignment'])\n",
    "        \n",
    "        if has_both_sides_same_line or has_left_but_used_before:\n",
    "            analysis['classification'] = 'inoput'\n",
    "        elif (all_occurrences and \n",
    "              all_occurrences[0]['type'] == 'left_assignment'):\n",
    "            # First occurrence is left assignment and doesn't meet inoput criteria\n",
    "            analysis['classification'] = 'output'\n",
    "        else:\n",
    "            # Any other usage pattern\n",
    "            analysis['classification'] = 'input'\n",
    "    \n",
    "    return usage_analysis\n",
    "\n",
    "# Main execution\n",
    "            \n",
    "    # Process both uploaded files\n",
    "\n",
    "for filename in files_to_process:\n",
    "    fullpath=search_path+filename\n",
    "    if os.path.exists(fullpath):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            sections = separate_fortran_file(fullpath)\n",
    "            print_summary(sections)\n",
    "            \n",
    "            # Optionally write separated files\n",
    "            #write_separated = input(f\"\\nWrite separated files for {filename}? (y/n): \").lower().strip()\n",
    "            #if write_separated == 'y':\n",
    "            #    write_separated_files(filename, sections)\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(sections['header'])\n",
    "            \n",
    "                for i, sub_funcs in enumerate(sections['subroutines']):\n",
    "                    routine_sects=extract_and_separate_subroutines_functions(sub_funcs['content'])\n",
    "                    f.write('\\n!'+'-'*100+'\\n')\n",
    "                    f.write(routine_sects[0]['separated']['part1'])\n",
    "                    f.write(\"\\n\")\n",
    "                    aliases=extract_aliases_from_associate(routine_sects[0]['separated']['part2'])\n",
    "                    if aliases:\n",
    "                        f.write('  associate('+' '*54+'&\\n')\n",
    "                    a_len,t_len,num_alias=0,0,0\n",
    "                    for alias in aliases:\n",
    "                        a_len=max(a_len,len(alias.strip()))\n",
    "                        t_len=max(t_len,len(aliases[alias].strip()))\n",
    "                        num_alias=num_alias+1\n",
    "                        \n",
    "                    usage_analysis=analyze_variable_usage_in_part3(routine_sects[0]['separated']['part3'], aliases)\n",
    "                    alias_v,target_v,label_v=[],[],[]\n",
    "                    for alias in aliases:   \n",
    "                        target=aliases[alias]\n",
    "                        label=usage_analysis[alias]['classification']\n",
    "                        alias_v.append(alias)\n",
    "                        target_v.append(target)\n",
    "                        label_v.append(label)\n",
    "                    k=0    \n",
    "                    for i in range(num_alias):    \n",
    "                        if label_v[i] == 'input':\n",
    "                            try:\n",
    "                                index = decl_varname.index(alias_v[i])                             \n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))                                \n",
    "                            except Exception as e:\n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                            k=k+1        \n",
    "    \n",
    "                    for i in range(num_alias):    \n",
    "                        if label_v[i] == 'inoput':\n",
    "                            try:\n",
    "                                index = decl_varname.index(alias_v[i])                             \n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))\n",
    "                            except Exception as e:\n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                            k=k+1            \n",
    "    \n",
    "    \n",
    "                    for i in range(num_alias):    \n",
    "                        if label_v[i] == 'output':\n",
    "                            try:\n",
    "                                index = decl_varname.index(alias_v[i])                             \n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}:{}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7,re.sub(r'\\s+', ' ',decl_comment[index])))\n",
    "                            except Exception as e:\n",
    "                                if k==num_alias-1:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}   & !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                                else:\n",
    "                                    f.write(\"    {:<{}} => {:<{}}  ,& !{:<{}}\\n\".format(alias_v[i], a_len, target_v[i], t_len,label_v[i],7))\n",
    "                            k=k+1            \n",
    "                    if aliases:\n",
    "                        f.write('  )\\n')\n",
    "                    f.write(routine_sects[0]['separated']['part3'])\n",
    "                    f.write('\\n')\n",
    "                f.write(sections['tail'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d627eb9-3ad7-4649-b583-88fade8bbbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
